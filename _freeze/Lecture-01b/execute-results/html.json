{
  "hash": "b7d22f5e4655f8ce6460179d52e7a89f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Lecture 01b - Introduction\nsubtitle: ENVX2001 Applied Statistical Methods\nauthor: Dr. Januar Harianto \ninstitute: The University of Sydney\ndate: last-modified # today | last-modified\ndate-format: \"MMM YYYY\" # see https://momentjs.com/docs/#/displaying/format/\nexecute:\n  cache: false\n  echo: true\neditor-options:\n  canonical: true\ntoc: true\ntoc-depth: 1\ntoc-title: Outline\n---\n\n\n\n\n\n# [Download PDF](Lecture-01b.pdf)\nClick above to access a .pdf version of this presentation.\n\n*Otherwise, continue navigating through the slides.*\n\n# Samples, populations and study design\n\n> \"To call in a statistician after the experiment has been done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.\" \n\n-- Sir Ronald Fisher\n\n## Revision\n\n- **Population**: the entire group of individuals or instances about whom we want to draw conclusions.\n- **Sample**: a *subset* of the population.\n\n::: {.fragment .incremental}\n### Parameter\n- A numerical **measure** that describes an aspect of a population.\n- *Not known* (unless we sample the entire population), therefore we **estimate** them using a **sample statistic**.\n- **What information does the *sample statistic* give about the *population parameter*, and how reliable is that information?**\n:::\n\n<!-- ::: {.fragment .incremental} -->\n<!-- ### Uncertainty -->\n\n<!-- - Populations are inherently variable - **random variation**. -->\n<!-- - Repeated samples from the same population will give different sample statistics - **sampling variation**. -->\n<!-- - **We want to be able to quantify this uncertainty in our estimates.** -->\n<!-- ::: -->\n\n\n## Confused?\n\nVisit the [ENVX Resources](https://github.com/ENVX-resources) organisation on GitHub.\n\n- [Probability distributions](https://github.com/ENVX-resources/ENVX1002-2024-Lecture-Topic03) (ENVX1002) - 2024 version\n- [Sampling distributions](https://github.com/ENVX-resources/ENVX1002-2024-Lecture-Topic04) (ENVX1002) - 2024 version\n\n\n::: {.fragment .callout-tip}\nYou will explore more experimental design principles next week, and in **Module 2**.\n:::\n\n# Designs: why do we care?\n\n> (On a failed experiment)\n>\n> *That is not an experiment you have there, that is an experience*.\n\n-- Sir Ronald Fisher\n\n## Measure everything?\n\nWhy not measure every individual in a population, instead of designing a sampling strategy?\n\n::: {.fragment}\n- **Impractical** to measure every individual in a population, and some populations are *infinite* -- practically impossible to measure all.\n- **Costly** to measure every individual in a population -- time, money, resources.\n- **Destructive** in many biological cases e.g. to measure the age of a plant, you may have to cut it down, so you want to respect the loss of life.\n:::\n\n::: {.fragment}\n::: {.callout-important}\nImportantly, sampling from a population -- when done correctly -- can give a **good estimate of the population parameter**, give or take some *uncertainty*. Apart from a census study, there should be no reason to measure every individual in a population.\n:::\n:::\n\n## Sampling designs\n\nCan be done in two general ways:\n\n1. **Observational study**\n2. **Controlled experiment**\n\nWhen designed correctly, both can give us a good estimate of the population parameter while saving time and resources.\n\n::: {.fragment}\n### Considerations\n\n- Samples should be **representative** of the population and **randomly** selected.\n- **Bias** can be introduced if the sampling design is not carefully considered.\n- **Confounding** variables can also affect the results.\n\nWe will explore these concepts in more detail over the next few weeks.\n:::\n\n## Observational study vs. controlled experiment\n\n| Aspect | Observational study | Controlled experiment |\n|--------|---------------------|-----------------------|\n| **Control** | No control over the variables of interest - **Mensurative** and **Absolute** | Control over the variables of interest - **Comparative** and **Manipulative** |\n| **Causation** | Cannot establish causation, but perhaps **association** | Can establish **causation** |\n| **Feasibility** | Can be done in many cases | May be destructive and cannot always be done |\n\n\n## Other designs exist\n\n- **Theoretical models** (e.g. mathematical models): useful for understanding the system, often used in ecology and epidemiology. No data collection.\n- **Simulation studies**: useful for figuring out experimental design and understanding the system. Some data collection may be involved to inform the model.\n- **Case studies**: Similar to observational studies, but often with a single case! Useful for understanding a unique situation, often used in medicine and psychology. No control over the variables of interest and sometimes no statistical inference is made.\n\n## Soil carbon {auto-animate=true}\n\n![](images/soilmap.png)\n\n\n## Soil carbon {auto-animate=true}\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n![](images/soilmap.png)\n:::\n\n::: {.column width=\"50%\"}\n\n### What is the best way to sample?\n\n- Sequestering carbon in soil is a potential way to mitigate climate change, and provides nutrients and resilience to crops. Worth $50/tonne if measured.\n- Collecting soil samples is costly and time-consuming, about $100/sample.\n- We want a way to estimate the soil carbon content in a large area - some kind of **summary statistic**.\n\n:::\n::::\n\n\n## Summary statistics\n\n::: {.fragment}\n### Central tendency\n\n- **Mean**: the average of the data.\n- **Median**: the middle value of the data.\n- **Mode**: the most frequent value in the data.\n\n:::\n\n::: {.fragment}\n### Variability\n\n- **Range**: the difference between the largest and smallest value.\n- **Interquartile range**: the difference between the 75th and 25th percentile.\n- **Variance**: the average of the squared differences from the mean.\n- **Standard deviation**: the square root of the variance.\n\n:::\n\n# Mean and standard deviation\n\n> Statistics always remind me of the fellow who drowned in a river whose average depth was only three feet (\\~0.9 m).\n\n-- *Woody Hayes, American football coach*\n\n## Mean and standard deviation\n\n- The most *common* measures of central tendency and variability.\n- Works well for **symmetric** and **unimodal** distributions, therefore the assumption is that the data is normally distributed.\n\n::: {.fragment}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\n\nggplot(\n    data = tibble(x = seq(0, 20, 0.01)),\n    aes(x = x)\n) +\n    stat_function(\n        fun = dchisq,\n        args = list(df = 5),\n        color = \"purple\", size = 1\n    ) +\n    # add vertical lines for mean, median and mode\n    geom_vline(xintercept = 5, color = \"red\", linetype = \"dashed\") +\n    geom_vline(xintercept = 4.35, color = \"blue\", linetype = \"dashed\") +\n    geom_vline(xintercept = 3, color = \"seagreen\", linetype = \"dashed\") +\n    # label the lines\n    annotate(\"text\", x = 3.6, y = 0.06, label = \"Mode\", color = \"seagreen\") +\n    annotate(\"text\", x = 5.6, y = 0.09, label = \"Mean\", color = \"red\") +\n    annotate(\"text\", x = 5.1, y = 0.03, label = \"Median\", color = \"blue\") +\n    labs(\n        x = \"x\",\n        y = \"Density\"\n    ) +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n- Not quite useful for **skewed** or **multimodal** distributions.\n\n:::\n\n\n## Arithmetric mean\n\n> Sum of all the values, divided by the number of values.\n\n::: {.fragment}\n### Population mean\nIf we measure the entire population, the population mean $\\mu$ is:\n\n$$ \\mu = \\frac{\\sum_{i=1}^{N} y_i}{N} $$\n\nwhere $y_i$ is the $i$th observation and $N$ is the number of individuals in the population.\n:::\n::: {.fragment}\n### Sample mean\nSample mean is based on the same principle, but we use $n$ instead of $N$ and $\\bar{y}$ instead of $\\mu$.\n\n$$ \\bar{y} = \\frac{\\sum_{i=1}^{n} y_i}{n} $$\n\nwhere $y_i$ is the $i$th observation and $n$ is the number of sample observations.\n:::\n\n## Variance\n\n::: {.fragment}\n\n> The average of the squared differences from the mean.\n\n:::\n\n::: {.fragment}\n#### Population variance:\n$$\\sigma^2 = \\frac{\\sum_{i=1}^{N} (y_i - \\mu)^2}{N}$$\n\n#### Sample variance\n$$s^2 = \\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}{n-1}$$\n\n:::\n\n## Standard deviation\n\n::: {.fragment}\n> The square root of the variance.\n\n:::\n\n::: {.fragment}\n#### Population standard deviation\n\n$$\\sigma = \\sqrt{\\sigma^2} = \\sqrt{\\frac{\\sum_{i=1}^{N} (y_i - \\mu)^2}{N}}$$\n\n#### Sample standard deviation\n\n$$s = \\sqrt{s^2} = \\sqrt{\\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}{n-1}}$$\n\n:::\n\n\n## Why $n-1$?\n\n- The sample variance and standard deviation calculations use $n-1$ in the denominator, not $n$.\n- This is called [**Bessel's correction**](https://en.wikipedia.org/wiki/Bessel%27s_correction).\n- It is used to correct the bias in the estimation of the population variance from a sample, **as $n$ number of observations have $n-1$ independent residuals**.\n  - *You will learn more about this -- and degrees of freedom -- in the next module*.\n\n\n## Soil carbon\n\n**Sampling design**: Soil carbon content was measured at 7 locations across the area. The amount at each location was 48, 56, 90, 78, 86, 71, 42 tonnes per hectare (t/ha).\n\n::: {.fragment}\n\n::: {.cell}\n\n```{.r .cell-code}\nsoil <- c(48, 56, 90, 78, 86, 71, 42)\nsoil\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 48 56 90 78 86 71 42\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.fragment}\n### Calculating mean and standard deviation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(soil)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 67.28571\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(soil)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 18.8566\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.fragment .callout-tip}\n## What do these numbers tell us? How confident are we that they represent the entire area?\n\n:::\n\n# The sampling distribution\n\n## Distributions\n\n- The **population distribution** is the distribution of all the individuals in the population.\n- From the population distribution, we can sample it to get a **sample distribution**.\n- If we summarise the sample distribution, we get a single value - the **sample statistic**.\n- The sample statistic is part of a **sampling distribution**, based on the idea that given unlimited resources, we could sample the population many times and calculate the sample statistic each time.\n\n::: {.fragment}\n\n### Example\n\n::: {.incremental}\n- We want to measure the mean height of trees in a forest, which contains 1000 trees. **1000 possible height values make up the population distribution**.\n- We can't measure all the trees, so we take a sample of 100 trees and calculate the average height.**100 height values make up the sample distribution**.\n- The mean height of the 100 trees is calculated. This is the **sample statistic** - a single value for the sample.\n- To make up the **sampling distribution**, we could repeat the process of taking a sample of 100 trees and calculating the mean height many times...\n:::\n:::\n\n\n## Distributions - visualised\n\n![Population, sample and sampling distributions. [Source](https://www.researchgate.net/figure/Population-Sample-and-Sampling-Distributions_fig2_267837095).](images/distributions.png){#fig-distributions fig-align=\"center\"}\n\n\n## How can distributions help us answer the question?\n\n> What information does the *sample statistic* give about the *population parameter*, and how reliable is that information?\n\n::: {.fragment}\nWe need to standardise the sample statistic to the *number of observations* in the sample.\n:::\n\n::: {.fragment}\n### Standard error\n\n$$SE = \\frac{s}{\\sqrt{n}}$$\n\nwhere $s$ is the sample standard deviation and $n$ is the number of observations in the sample.\n\n:::\n\n::: {.incremental}\n- The standard deviation value is *standardised* to the number of observations in the sample.\n- Tells us how much the sample statistic varies from sample to sample, i.e. **how well we know the mean**.\n- If standard error is \"small\", we are more confident in the sample statistic -- **more on this next week**.\n:::\n\n## Effect of sample size\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidymodels)\nlibrary(patchwork)\nset.seed(642)\n\nheights <- tibble(heights = rnorm(1000, 1.99, 1))\npopmean <- mean(heights$heights)\nsample_sizes <- c(2, 5, 25, 100)\nn <- length(sample_sizes)\n\nheights <- tibble(heights = rgamma(1000, shape = 2, scale = 1))\nsample_sizes <- c(2, 5, 25, 100)\nn <- length(sample_sizes)\n\nplots <- lapply(sample_sizes, function(size) {\n    df <- heights |>\n        rep_sample_n(size = size, reps = 2000) |>\n        group_by(replicate) |>\n        summarise(xbar = mean(heights))\n\n    mean_xbar <- mean(df$xbar)\n\n    ggplot(df, aes(x = xbar)) +\n        geom_histogram(fill = \"orangered\", alpha = 0.5, bins = 50) +\n        geom_vline(aes(xintercept = mean_xbar), color = \"blue\", linetype = \"dashed\") +\n        geom_text(aes(x = mean_xbar, label = sprintf(\"%.2f\", mean_xbar), y = Inf), hjust = -0.1, vjust = 2, color = \"blue\") +\n        ggtitle(paste0(\"Sample Size: \", size)) +\n        xlab(\"Mean height (m)\") +\n        xlim(-3, 8) +\n        theme_bw()\n})\nwrap_plots(plots)\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n\n- Increased sample size leads to a more accurate estimate of the population mean, reflected by the **narrower distribution** of the sample mean, which is captured by the **standard error**.\n\n## Effect of variability\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(1221)\n\n# Define a function to generate ggplot objects\ngenerate_plot <- function(sd) {\n    data <- rnorm(500, 1.99, sd)\n    p <- ggplot(data = tibble(x = data), aes(x = x)) +\n        geom_histogram(fill = \"orangered\", alpha = 0.5, bins = 50) +\n        ggtitle(paste(\"SD =\", sd)) +\n        xlim(-100, 100) +\n        theme_bw()\n    return(p)\n}\n\n# Apply the function to a list of standard deviations\nsds <- c(3, 6, 15, 25)\nplots <- lapply(sds, generate_plot)\n\n# Wrap the plots\nwrap_plots(plots)\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n- Increased variability leads to a wider distribution of the sample mean (i.e. less precision), which is *also* reflected by the **standard error**.\n\n# Central limit theorem\n\n> I know of scarcely anything so apt to impress the imagination as the **wonderful form of cosmic order** expressed by the Central Limit Theorem. The law would have been personified by the Greeks and deified, if they had known of it.‚Äù\n\n-- Sir Francis Galton, 1889, Natural Inheritance* (emphasis added)\n\n\n## CLT\n\n- A fundamental theorem in statistics.\n- Regardless of the shape of the population distribution, the sampling distribution of the **sample mean** will be approximately normally distributed **if the sample size is large enough**.\n- Because of this, we can make predictions about the population by assuming that the sampling distribution is normally distributed -- **a core assumption in many statistical tests**.\n\n## Example\n\n\n\n::: {.cell .custom4060 output-location='column'}\n\n```{.r .cell-code}\nset.seed(239)\nlibrary(ggplot2)\nlibrary(dplyr)\n# Generate a skewed distribution\nskewed <- tibble(\n    x = rgamma(1000, shape = 2, scale = 1)\n)\n\n# plot in ggplot2\nggplot(data = skewed, aes(x = x)) +\n    geom_histogram(\n        fill = \"orangered\",\n        alpha = 0.5, bins = 50\n    ) +\n    xlab(\"Height (m)\") +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n- Skewed population distribution for tree heights.\n- We want to estimate the mean height of the trees in the forest.\n\n## 1 sample (no summary statistic)\n\n\n\n::: {.cell .custom4060 output-location='column'}\n\n```{.r .cell-code}\nskewed |>\n    infer::rep_sample_n(\n        size = 1,\n        reps = 1000\n    ) |>\n    group_by(replicate) |>\n    summarise(xbar = mean(x)) |>\n    ggplot(aes(x = xbar)) +\n    geom_density(\n        fill = \"orangered\",\n        alpha = 0.5, bins = 50\n    ) +\n    xlim(0, 10) +\n    xlab(\"Mean height (m)\") +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n- A single random sample per calculated mean, repeated 1000 times, gives us a distribution of sample means that will likely mirrors the population distribution.\n\n## 2 samples\n\n\n\n::: {.cell .custom4060 output-location='column'}\n\n```{.r .cell-code}\nskewed |>\n    infer::rep_sample_n(\n        size = 2,\n        reps = 1000\n    ) |>\n    group_by(replicate) |>\n    summarise(xbar = mean(x)) |>\n    ggplot(aes(x = xbar)) +\n    geom_density(\n        fill = \"orangered\",\n        alpha = 0.5, bins = 50\n    ) +\n    xlim(0, 10) +\n    xlab(\"Mean height (m)\") +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n- Two random samples per calculated mean, repeated 1000 times.\n- The distribution of sample means is starting to look more like a normal distribution.\n\n\n## 5 samples\n\n\n::: {.cell .custom4060 output-location='column'}\n\n```{.r .cell-code}\nskewed |>\n    infer::rep_sample_n(\n        size = 5,\n        reps = 1000\n    ) |>\n    group_by(replicate) |>\n    summarise(xbar = mean(x)) |>\n    ggplot(aes(x = xbar)) +\n    geom_density(\n        fill = \"orangered\",\n        alpha = 0.5, bins = 50\n    ) +\n    xlim(0, 10) +\n    xlab(\"Mean height (m)\") +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n- Five random samples per calculated mean, repeated 1000 times.\n- Not only is the distribution of sample means starting to look more like a normal distribution, but the standard error is also getting smaller.\n\n## 30 samples\n\n\n::: {.cell .custom4060 output-location='column'}\n\n```{.r .cell-code}\nskewed |>\n    infer::rep_sample_n(\n        size = 30,\n        reps = 1000\n    ) |>\n    group_by(replicate) |>\n    summarise(xbar = mean(x)) |>\n    ggplot(aes(x = xbar)) +\n    geom_density(\n        fill = \"orangered\",\n        alpha = 0.5, bins = 50\n    ) +\n    xlim(0, 10) +\n    xlab(\"Mean height (m)\") +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n- Thirty random samples per calculated mean, repeated 1000 times.\n- The distribution of sample means is very close to a normal distribution.\n\n## 50 samples\n\n\n::: {.cell .custom4060 output-location='column'}\n\n```{.r .cell-code}\nskewed |>\n    infer::rep_sample_n(\n        size = 50,\n        reps = 1000\n    ) |>\n    group_by(replicate) |>\n    summarise(xbar = mean(x)) |>\n    ggplot(aes(x = xbar)) +\n    geom_density(\n        fill = \"orangered\",\n        alpha = 0.5, bins = 50\n    ) +\n    xlim(0, 10) +\n    xlab(\"Mean height (m)\") +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n- Fifty random samples per calculated mean, repeated 1000 times.\n- **How many samples is enough?**\n\n## How many samples is enough?\n\n::: {.incremental}\n- If $n$ is large enough, the sampling distribution of the sample mean will be approximately normally distributed - **allowing us to use the normal distribution to make inferences about the population!**\n- **How large is large enough?**\n  - **Rule of thumb**: $n \\geq 30$ is often used, but this is not a hard and fast rule.\n  - **Depends on the population distribution**: if the population distribution is normal, the sampling distribution will be normal for any $n$.\n  - **Depends on the variability**: if the population distribution is highly variable, a larger $n$ is needed to get a normal sampling distribution.\n:::\n\n## Thanks\n\n### Questions?\n\nThis presentation is based on the [SOLES Quarto reveal.js template][soles-revealjs] and is licensed under a [Creative Commons Attribution 4.0 International License][cc-by] \n\n\n<!-- Links -->\n[soles-revealjs]: https://github.com/usyd-soles-edu/soles-revealjs\n[cc-by]: http://creativecommons.org/licenses/by/4.0/\n",
    "supporting": [
      "Lecture-01b_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}